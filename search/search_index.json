{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to AutoML \u00b6 Automated Machine Learning , or AutoML for short, is a novel and expanding field in the intersection of machine learning, optimization, and software engineering. It's purpose is to progresively automate the most common (and often boring) tasks of conventional ML workflows. Tasks such as data preprocessing, feature extraction and selection, model selection, hyperparameter tunning, model validation, deployment, and monitoring. Despite its novelty, AutoML has become prominent in the last few years, as more profesionals from every field get into machine learning, often without a solid background in machine learning and no time to learn all the necessary theory. The following is a beginner-friendly introduction to the AutoML field. Our purpose is not to survey all the deep theory behind AutoML, but rather to provide an entry-point for newcommers, interested both in the research aspects and the practical aspects of AutoML. To narrow our focus, we decided to analize a set of practical AutoML systems, and extend outwards into the underlying theory guided by the paradigms and concepts that are most commonly used in these practical scenarios. To consider an AutoML system for inclusion, we defined a set of loose criteria, mostly regarding its status as a software product intended for broad use rather than, say, a reference implementation of a novel technique for research purposes. Thus, we consider a broad range of systems, both open-source and commercial, in different levels of maturity and with widely different features, as long as they fit the above criteria. The list of AutoML systems considered in this survey, and the features that are evaluated, are the result of a colaborative effort . Based on the analized systems we defined a set of conceptual features that help mapping the AutoML field, at least from a practical point-of-view. These features involve both internal and external characteristics of the systems. The internal characteristics refer to how the system works, e.g., the techniques it uses for optimizing and searching machine learning pipelines, and the types of hyperparameters it can represent. The external characteristics refer to the types of tasks that can be solved with the system and the way the user interacts with it. Based on these characteristics, we created a taxonomy of commonly used theoretical concepts, principle, and paradigims in the AutoML field, which guide this survey. We provide follow-up references on many of the topics we cover. AutoML in a nutshell \u00b6 At its core, AutoML is about providing tools to automate the process of designing, training, validating, and deploying a machine learning pipeline for a given problem. ML pipelines come in varied flavour, but they are often composed of a set of atomic operators (e.g., a feature selection algorithm, or a specific machine learning model), each of which can be configured by one or more hyperparameters (e.g., the number and type of layers and neurons per layer in a neural network, or the regularization factor in logistic regression). Each operator performs some task, often associated with an input data source, and producing an output that is fed to subsequent operators. The objective is to find a pipeline that is optimal, or close to optimal, in solving a given machine learning problem, among a set of posible pipelines. Thus, we can see an AutoML system, on a broad perspective, as a computational solution that receives a machine learning problem definition of some kind~(e.g., classification, regression, clustering) and associated training data, and it outputs a suitable machine learning pipeline, composed of one or more atomic operators, that is close to optimal in solve that problem according to some predefined performance metric(s). The core of any AutoML system is a machine learning pipeline optimization engine . Three key components can be identified in any such engine: a search space, a search strategy, and a performance estimation function. Search space \u00b6 The search space defines which types of pipelines are available. It is often restricted by an underlying machine learning library (or libraries) that support the actual implementation of said pipelines. For example, AutoML systems based on scikit-learn can only output pipelines composed of scikit-learn estimators and transformers, while AutoML systems based on keras or pytorch can output pipelines that correspond to neural network architectures. The search space also defines which hyperparameters are optimized, and to what extent. Search strategy \u00b6 A search strategy is an algorithm to efficiently explore a given search space and find the optimal pipelines. The search space can be arbitrarily big, exponential in size with respect to the number of available operators, and potentially unbounded in some of the hyperparameters (e.g., you can add as many layers as you want to a neural network). In machine learning, the difference between a useless solution and a highly effective one often boils down to the use of one type of algorithm over another, or even to their specific configurations. An effective search strategy must be able to quickly focus on promising regions of the search space, while still ensuring a thorough enough exploration to avoid missing potentially Performance estimation function \u00b6 Finally, the performance estimation function measures the expected performance of any given pipeline in the task at hand. The simplest performance estimation consists in evaluating the pipeline in a validation dataset, but evaluating machine learning algorithms is a costly task, especially when using models with millions or billions of parameters, like the largest neural networks. Thus, we often want to estimate this performance either by evaluating in smaller sets or by creating surrogate functions that approximate it. Furthermore, we often want to optimize more than one performance indicator, which may be in contradiction, such as accuracy versus model complexity (to reduce inference time). Flavours of AutoML \u00b6 Over the years several different conceptualizations of AutoML have been proposed, some more flexible than others. We'll review next some of the most common flavours of AutoML from a high-level point of view, without dwelving too deep into their specifities. Afterwards, when we have introduced all the building blocks of the AutoML process, we'll be able to look back at these definitions in more details. Hyperparameter optimization \u00b6 Hyperparameter optimization is a huge field, with lots of internal research. Full model selection \u00b6 Combined algorithm selection and hyperparameter optimization \u00b6 Neural architecture search \u00b6 Heterogenous automated machine learning \u00b6 What's next \u00b6 The three components mentioned above (search space, search strategy, and performance estimation function) make up the core of the \"internal characteristics\" of an AutoML system. In this survey, we are also interested in \"external\" characteristics, such as the types of machine learning tasks that can be solved, the interfaces by which users interact with the system, the steps of the machine learning workflow that are covered, and other software engineering concerns. In the next few sections we'll review the basic theory behind the core AutoML process. We'll introduce the most common types of search spaces and strategies, and some interesting performance estimation functions. Finally, we'll present a comparison of many practical AutoML systems in terms of all of these characteristics, and provide some rules guidelines for choosing an appropiate tool for a given task.","title":"Introduction"},{"location":"#introduction-to-automl","text":"Automated Machine Learning , or AutoML for short, is a novel and expanding field in the intersection of machine learning, optimization, and software engineering. It's purpose is to progresively automate the most common (and often boring) tasks of conventional ML workflows. Tasks such as data preprocessing, feature extraction and selection, model selection, hyperparameter tunning, model validation, deployment, and monitoring. Despite its novelty, AutoML has become prominent in the last few years, as more profesionals from every field get into machine learning, often without a solid background in machine learning and no time to learn all the necessary theory. The following is a beginner-friendly introduction to the AutoML field. Our purpose is not to survey all the deep theory behind AutoML, but rather to provide an entry-point for newcommers, interested both in the research aspects and the practical aspects of AutoML. To narrow our focus, we decided to analize a set of practical AutoML systems, and extend outwards into the underlying theory guided by the paradigms and concepts that are most commonly used in these practical scenarios. To consider an AutoML system for inclusion, we defined a set of loose criteria, mostly regarding its status as a software product intended for broad use rather than, say, a reference implementation of a novel technique for research purposes. Thus, we consider a broad range of systems, both open-source and commercial, in different levels of maturity and with widely different features, as long as they fit the above criteria. The list of AutoML systems considered in this survey, and the features that are evaluated, are the result of a colaborative effort . Based on the analized systems we defined a set of conceptual features that help mapping the AutoML field, at least from a practical point-of-view. These features involve both internal and external characteristics of the systems. The internal characteristics refer to how the system works, e.g., the techniques it uses for optimizing and searching machine learning pipelines, and the types of hyperparameters it can represent. The external characteristics refer to the types of tasks that can be solved with the system and the way the user interacts with it. Based on these characteristics, we created a taxonomy of commonly used theoretical concepts, principle, and paradigims in the AutoML field, which guide this survey. We provide follow-up references on many of the topics we cover.","title":"Introduction to AutoML"},{"location":"#automl-in-a-nutshell","text":"At its core, AutoML is about providing tools to automate the process of designing, training, validating, and deploying a machine learning pipeline for a given problem. ML pipelines come in varied flavour, but they are often composed of a set of atomic operators (e.g., a feature selection algorithm, or a specific machine learning model), each of which can be configured by one or more hyperparameters (e.g., the number and type of layers and neurons per layer in a neural network, or the regularization factor in logistic regression). Each operator performs some task, often associated with an input data source, and producing an output that is fed to subsequent operators. The objective is to find a pipeline that is optimal, or close to optimal, in solving a given machine learning problem, among a set of posible pipelines. Thus, we can see an AutoML system, on a broad perspective, as a computational solution that receives a machine learning problem definition of some kind~(e.g., classification, regression, clustering) and associated training data, and it outputs a suitable machine learning pipeline, composed of one or more atomic operators, that is close to optimal in solve that problem according to some predefined performance metric(s). The core of any AutoML system is a machine learning pipeline optimization engine . Three key components can be identified in any such engine: a search space, a search strategy, and a performance estimation function.","title":"AutoML in a nutshell"},{"location":"#search-space","text":"The search space defines which types of pipelines are available. It is often restricted by an underlying machine learning library (or libraries) that support the actual implementation of said pipelines. For example, AutoML systems based on scikit-learn can only output pipelines composed of scikit-learn estimators and transformers, while AutoML systems based on keras or pytorch can output pipelines that correspond to neural network architectures. The search space also defines which hyperparameters are optimized, and to what extent.","title":"Search space"},{"location":"#search-strategy","text":"A search strategy is an algorithm to efficiently explore a given search space and find the optimal pipelines. The search space can be arbitrarily big, exponential in size with respect to the number of available operators, and potentially unbounded in some of the hyperparameters (e.g., you can add as many layers as you want to a neural network). In machine learning, the difference between a useless solution and a highly effective one often boils down to the use of one type of algorithm over another, or even to their specific configurations. An effective search strategy must be able to quickly focus on promising regions of the search space, while still ensuring a thorough enough exploration to avoid missing potentially","title":"Search strategy"},{"location":"#performance-estimation-function","text":"Finally, the performance estimation function measures the expected performance of any given pipeline in the task at hand. The simplest performance estimation consists in evaluating the pipeline in a validation dataset, but evaluating machine learning algorithms is a costly task, especially when using models with millions or billions of parameters, like the largest neural networks. Thus, we often want to estimate this performance either by evaluating in smaller sets or by creating surrogate functions that approximate it. Furthermore, we often want to optimize more than one performance indicator, which may be in contradiction, such as accuracy versus model complexity (to reduce inference time).","title":"Performance estimation function"},{"location":"#flavours-of-automl","text":"Over the years several different conceptualizations of AutoML have been proposed, some more flexible than others. We'll review next some of the most common flavours of AutoML from a high-level point of view, without dwelving too deep into their specifities. Afterwards, when we have introduced all the building blocks of the AutoML process, we'll be able to look back at these definitions in more details.","title":"Flavours of AutoML"},{"location":"#hyperparameter-optimization","text":"Hyperparameter optimization is a huge field, with lots of internal research.","title":"Hyperparameter optimization"},{"location":"#full-model-selection","text":"","title":"Full model selection"},{"location":"#combined-algorithm-selection-and-hyperparameter-optimization","text":"","title":"Combined algorithm selection and hyperparameter optimization"},{"location":"#neural-architecture-search","text":"","title":"Neural architecture search"},{"location":"#heterogenous-automated-machine-learning","text":"","title":"Heterogenous automated machine learning"},{"location":"#whats-next","text":"The three components mentioned above (search space, search strategy, and performance estimation function) make up the core of the \"internal characteristics\" of an AutoML system. In this survey, we are also interested in \"external\" characteristics, such as the types of machine learning tasks that can be solved, the interfaces by which users interact with the system, the steps of the machine learning workflow that are covered, and other software engineering concerns. In the next few sections we'll review the basic theory behind the core AutoML process. We'll introduce the most common types of search spaces and strategies, and some interesting performance estimation functions. Finally, we'll present a comparison of many practical AutoML systems in terms of all of these characteristics, and provide some rules guidelines for choosing an appropiate tool for a given task.","title":"What's next"},{"location":"bayesian_strategy_examples/","text":"Auto-Keras , AutoPytorch , AutoSklearn , AutoWeka , Azure ML , Hyperopt-Sklearn , KNIME AutoML , Vertex AI .","title":"Bayesian strategy examples"},{"location":"comparison/","text":"Comparison between popular AutoML systems \u00b6","title":"Comparison"},{"location":"comparison/#comparison-between-popular-automl-systems","text":"","title":"Comparison between popular AutoML systems"},{"location":"constructive_strategy_examples/","text":"AutoGluon .","title":"Constructive strategy examples"},{"location":"evolutionary_strategy_examples/","text":"AutoGOAL , RECIPE , TPOT .","title":"Evolutionary strategy examples"},{"location":"fixed_pipeline_examples/","text":"AutoGOAL , AutoGluon , AutoNLP , AutoSklearn , AutoWeka , Azure ML , Hyperopt-Sklearn , KNIME AutoML , ML-Plan .","title":"Fixed pipeline examples"},{"location":"gradient_descent_strategy_examples/","text":"Hyperopt-Sklearn , KNIME AutoML .","title":"Gradient descent strategy examples"},{"location":"graph_pipeline_examples/","text":"Auto-Keras , AutoGOAL , TPOT , TransmogrifAI .","title":"Graph pipeline examples"},{"location":"grid_strategy_examples/","text":"","title":"Grid strategy examples"},{"location":"hill_climbing_strategy_examples/","text":"","title":"Hill climbing strategy examples"},{"location":"hyperband_strategy_examples/","text":"","title":"Hyperband strategy examples"},{"location":"linear_pipeline_examples/","text":"AutoGOAL , RECIPE .","title":"Linear pipeline examples"},{"location":"monte_carlo_strategy_examples/","text":"ML-Plan .","title":"Monte carlo strategy examples"},{"location":"performance-estimation/","text":"Performance estimation functions \u00b6 Cross-validation \u00b6 Multi-fidelity \u00b6 \u00b6","title":"Performance Estimation"},{"location":"performance-estimation/#performance-estimation-functions","text":"","title":"Performance estimation functions"},{"location":"performance-estimation/#cross-validation","text":"","title":"Cross-validation"},{"location":"performance-estimation/#multi-fidelity","text":"","title":"Multi-fidelity"},{"location":"performance-estimation/#_1","text":"","title":""},{"location":"random_strategy_examples/","text":"AutoGOAL , Azure ML , H2O AutoML , Hyperopt-Sklearn , KNIME AutoML , TransmogrifAI , Vertex AI .","title":"Random strategy examples"},{"location":"reinforcement_learning_strategy_examples/","text":"","title":"Reinforcement learning strategy examples"},{"location":"search-space/","text":"Search spaces \u00b6 In the context of AutoML, a search space is an implicitly- or explicitly-defined collection of machine learning pipelines, among which to search for a suitable solution to a given machine learning problem. The search space in any given AutoML system ultimately defines which solutions are possible at all. To characterize common search spaces, we'll focus first on the building blocks of any search space: operators , hyperparameters , and pipelines . Then we'll look at common characteristics that can help us analize and compare the search spaces in current AutoML systems. Operators \u00b6 An operator is any atomic component that performs a given function in a machine learning pipeline: e.g., a tokenization algorithm, a feature selector, or a classification model. For the purpose of AutoML, operators are often black-box; that is, we don't care about their internal structure, we just care about whether they can fit in any given pipeline. Operators are thus often characterized by input, output, and a set of hyperparameters . The input and output define how that operator interacts with other operators in any given pipeline. This is not as simple as defining an input and output type , since often types, at least in their conventional definition in programming languages (i.e., independent types), are insufficient to completely characterize whether an operator can act on a given input. For example, most algorithms in scikit-learn take matrices as input, but some can only act on dense matrices, while others work for both dense and sparse matrices. Similarly, in NAS, most operators are neural layers, which all receive tensors as inputs. However, they work on specific tensor shapes , and we cannot connect a layer \\(L_1\\) to another layer \\(L_2\\) unless their output and input shapes match, respectively. Therefore, to completely characterize an operator, an AutoML systems needs and implict or explicit typing system that is able to capture these restrictions. The more flexible the pipeline representation and the more varied the types of operators involved, the more sophisticated the typing system should be. In fixed-size pipelines , for example, it often suffices to consider that every operator in each step has a definite input and output type. However, linear or graph-based pipelines need explicit or implicit restrictions about which operators can connect to each other. Models \u00b6 One special type of operators are models, which have that have internal parameters which are adjusted from training data. For example, in a decision tree classifier, the structure of the tree is adjusted such that it maximizes the probability of classifying correctly all the elements in the training set. The difference between models and the other operators is important because every machine learning pipeline ultimately fits one or more models. All the remaining operators are there for secundary, even if often crucial, tasks, such as feature preprocessing or dimensionality reduction. Models break the operator as a black-box illusion in one important sense: they have two modes of operation that must be dealt with explicitely in the AutoML engine, training, and prediction. Contrary to the other, model-free operators, classifiers and regressors must be run once on training data to adjust their parameters, and only then can they be actually used on new data. Thus, all AutoML systems must somehow deal with this two-mode operation issue. The most common strategy is to consider all operators to work in these two modes, with model-free operators just ignoring whatever mode they're in. Hyperparameters \u00b6 Hyperparameters are the tunable values of any operator that are not adjusted from data, but must be decided with a data-independent strategy. Examples include the number of layers or the activation functions in a neural network, the regularization strength in a linear classifier, or the maximum depth in a tree-based model. In a sense, the whole purpose of AutoML can be defined as finding the optimal configuration for all the hyperparameters involved in a set of selected operators. In fact, some AutoML paradigms are built entirely on top of the hyperparameter optimzation conceptualization. Paradigms like CASH consider the selection of operators as just another type of categorical hyperparameter . Continuous and discrete hyperparameters \u00b6 Categorical hyperparameters \u00b6 Conditional hyperparameters \u00b6 Pipelines \u00b6 According to how flexible these pipelines are, we can identify four basic types: Single model pipelines \u00b6 When a single model is trained end-to-end, which is often an estimator (e.g., a classifier or regressor). An example is training a neural network end-to-end for image classification, or a linear regression model for price estimation on tabular data. Examples: AutoGOAL . Fixed-size pipelines \u00b6 When a few fixed atomic steps are considered, e.g., data preprocessing, feature selection, dimensionality reduction, and classification. In each step, several different algorithms with they respective hyperparameters can be considered. Examples: AutoGOAL , AutoGluon , AutoNLP , AutoSklearn , AutoWeka , Azure ML , Hyperopt-Sklearn , KNIME AutoML , ML-Plan . Linear pipelines \u00b6 Examples: AutoGOAL , RECIPE . Graph-based pipelines \u00b6 Examples: Auto-Keras , AutoGOAL , TPOT , TransmogrifAI . Other features of search spaces \u00b6 Hierarchical spaces \u00b6 Probabilistic spaces \u00b6 Differentiable spaces \u00b6 Implicit versus explicit spaces \u00b6","title":"Search Space"},{"location":"search-space/#search-spaces","text":"In the context of AutoML, a search space is an implicitly- or explicitly-defined collection of machine learning pipelines, among which to search for a suitable solution to a given machine learning problem. The search space in any given AutoML system ultimately defines which solutions are possible at all. To characterize common search spaces, we'll focus first on the building blocks of any search space: operators , hyperparameters , and pipelines . Then we'll look at common characteristics that can help us analize and compare the search spaces in current AutoML systems.","title":"Search spaces"},{"location":"search-space/#operators","text":"An operator is any atomic component that performs a given function in a machine learning pipeline: e.g., a tokenization algorithm, a feature selector, or a classification model. For the purpose of AutoML, operators are often black-box; that is, we don't care about their internal structure, we just care about whether they can fit in any given pipeline. Operators are thus often characterized by input, output, and a set of hyperparameters . The input and output define how that operator interacts with other operators in any given pipeline. This is not as simple as defining an input and output type , since often types, at least in their conventional definition in programming languages (i.e., independent types), are insufficient to completely characterize whether an operator can act on a given input. For example, most algorithms in scikit-learn take matrices as input, but some can only act on dense matrices, while others work for both dense and sparse matrices. Similarly, in NAS, most operators are neural layers, which all receive tensors as inputs. However, they work on specific tensor shapes , and we cannot connect a layer \\(L_1\\) to another layer \\(L_2\\) unless their output and input shapes match, respectively. Therefore, to completely characterize an operator, an AutoML systems needs and implict or explicit typing system that is able to capture these restrictions. The more flexible the pipeline representation and the more varied the types of operators involved, the more sophisticated the typing system should be. In fixed-size pipelines , for example, it often suffices to consider that every operator in each step has a definite input and output type. However, linear or graph-based pipelines need explicit or implicit restrictions about which operators can connect to each other.","title":"Operators"},{"location":"search-space/#models","text":"One special type of operators are models, which have that have internal parameters which are adjusted from training data. For example, in a decision tree classifier, the structure of the tree is adjusted such that it maximizes the probability of classifying correctly all the elements in the training set. The difference between models and the other operators is important because every machine learning pipeline ultimately fits one or more models. All the remaining operators are there for secundary, even if often crucial, tasks, such as feature preprocessing or dimensionality reduction. Models break the operator as a black-box illusion in one important sense: they have two modes of operation that must be dealt with explicitely in the AutoML engine, training, and prediction. Contrary to the other, model-free operators, classifiers and regressors must be run once on training data to adjust their parameters, and only then can they be actually used on new data. Thus, all AutoML systems must somehow deal with this two-mode operation issue. The most common strategy is to consider all operators to work in these two modes, with model-free operators just ignoring whatever mode they're in.","title":"Models"},{"location":"search-space/#hyperparameters","text":"Hyperparameters are the tunable values of any operator that are not adjusted from data, but must be decided with a data-independent strategy. Examples include the number of layers or the activation functions in a neural network, the regularization strength in a linear classifier, or the maximum depth in a tree-based model. In a sense, the whole purpose of AutoML can be defined as finding the optimal configuration for all the hyperparameters involved in a set of selected operators. In fact, some AutoML paradigms are built entirely on top of the hyperparameter optimzation conceptualization. Paradigms like CASH consider the selection of operators as just another type of categorical hyperparameter .","title":"Hyperparameters"},{"location":"search-space/#continuous-and-discrete-hyperparameters","text":"","title":"Continuous and discrete hyperparameters"},{"location":"search-space/#categorical-hyperparameters","text":"","title":"Categorical hyperparameters"},{"location":"search-space/#conditional-hyperparameters","text":"","title":"Conditional hyperparameters"},{"location":"search-space/#pipelines","text":"According to how flexible these pipelines are, we can identify four basic types:","title":"Pipelines"},{"location":"search-space/#single-model-pipelines","text":"When a single model is trained end-to-end, which is often an estimator (e.g., a classifier or regressor). An example is training a neural network end-to-end for image classification, or a linear regression model for price estimation on tabular data. Examples: AutoGOAL .","title":"Single model pipelines"},{"location":"search-space/#fixed-size-pipelines","text":"When a few fixed atomic steps are considered, e.g., data preprocessing, feature selection, dimensionality reduction, and classification. In each step, several different algorithms with they respective hyperparameters can be considered. Examples: AutoGOAL , AutoGluon , AutoNLP , AutoSklearn , AutoWeka , Azure ML , Hyperopt-Sklearn , KNIME AutoML , ML-Plan .","title":"Fixed-size pipelines"},{"location":"search-space/#linear-pipelines","text":"Examples: AutoGOAL , RECIPE .","title":"Linear pipelines"},{"location":"search-space/#graph-based-pipelines","text":"Examples: Auto-Keras , AutoGOAL , TPOT , TransmogrifAI .","title":"Graph-based pipelines"},{"location":"search-space/#other-features-of-search-spaces","text":"","title":"Other features of search spaces"},{"location":"search-space/#hierarchical-spaces","text":"","title":"Hierarchical spaces"},{"location":"search-space/#probabilistic-spaces","text":"","title":"Probabilistic spaces"},{"location":"search-space/#differentiable-spaces","text":"","title":"Differentiable spaces"},{"location":"search-space/#implicit-versus-explicit-spaces","text":"","title":"Implicit versus explicit spaces"},{"location":"search-strategy/","text":"Search strategies \u00b6 Basic search strategies \u00b6 Random search \u00b6 Examples: AutoGOAL , Azure ML , H2O AutoML , Hyperopt-Sklearn , KNIME AutoML , TransmogrifAI , Vertex AI . Grid search \u00b6 Examples: Hill climbing \u00b6 Examples: Evolutionary search \u00b6 Examples: AutoGOAL , RECIPE , TPOT . Bayesian optimization \u00b6 Examples: Auto-Keras , AutoPytorch , AutoSklearn , AutoWeka , Azure ML , Hyperopt-Sklearn , KNIME AutoML , Vertex AI . Reinforced learning \u00b6 Examples: Constructive methods \u00b6 Examples: AutoGluon . Monte Carlo tree search \u00b6 Examples: ML-Plan . Meta-learning \u00b6 Meta-features \u00b6 Portfolios \u00b6 Warm starting \u00b6","title":"Search Strategies"},{"location":"search-strategy/#search-strategies","text":"","title":"Search strategies"},{"location":"search-strategy/#basic-search-strategies","text":"","title":"Basic search strategies"},{"location":"search-strategy/#random-search","text":"Examples: AutoGOAL , Azure ML , H2O AutoML , Hyperopt-Sklearn , KNIME AutoML , TransmogrifAI , Vertex AI .","title":"Random search"},{"location":"search-strategy/#grid-search","text":"Examples:","title":"Grid search"},{"location":"search-strategy/#hill-climbing","text":"Examples:","title":"Hill climbing"},{"location":"search-strategy/#evolutionary-search","text":"Examples: AutoGOAL , RECIPE , TPOT .","title":"Evolutionary search"},{"location":"search-strategy/#bayesian-optimization","text":"Examples: Auto-Keras , AutoPytorch , AutoSklearn , AutoWeka , Azure ML , Hyperopt-Sklearn , KNIME AutoML , Vertex AI .","title":"Bayesian optimization"},{"location":"search-strategy/#reinforced-learning","text":"Examples:","title":"Reinforced learning"},{"location":"search-strategy/#constructive-methods","text":"Examples: AutoGluon .","title":"Constructive methods"},{"location":"search-strategy/#monte-carlo-tree-search","text":"Examples: ML-Plan .","title":"Monte Carlo tree search"},{"location":"search-strategy/#meta-learning","text":"","title":"Meta-learning"},{"location":"search-strategy/#meta-features","text":"","title":"Meta-features"},{"location":"search-strategy/#portfolios","text":"","title":"Portfolios"},{"location":"search-strategy/#warm-starting","text":"","title":"Warm starting"},{"location":"single_pipeline_examples/","text":"AutoGOAL .","title":"Single pipeline examples"},{"location":"systems/","text":"A list of popular AutoML systems \u00b6 This section lists all the AutoML systems considered for the present survey and the associated metadata. A description of the characteristics considered for each system can be found in the Comparison section . Help We need your help! If you want to contribute to this section, you can either edit any of the systems' characteristics by clicking on each section's badge, or add new systems in our repository. AI Builder \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Auto-Keras \u00b6 Auto-Keras focuses on neural architecture search, using bayesian optimization with a custom acquistion function based on network morphism. Presents a high-level API inspired by scikit-learn , where users only need to select a task-specific model; and a low-level API where users can completely customize a neural search space. Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoGOAL \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoGluon \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoNLP \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoPytorch \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoSklearn \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoWeka \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Azure ML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture H2O AutoML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Hyperopt-Sklearn \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture KNIME AutoML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture ML-Plan \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture RECIPE \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture TPOT \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture TransmogrifAI \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Vertex AI \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Systems"},{"location":"systems/#a-list-of-popular-automl-systems","text":"This section lists all the AutoML systems considered for the present survey and the associated metadata. A description of the characteristics considered for each system can be found in the Comparison section . Help We need your help! If you want to contribute to this section, you can either edit any of the systems' characteristics by clicking on each section's badge, or add new systems in our repository.","title":"A list of popular AutoML systems"},{"location":"systems/#ai-builder","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AI Builder"},{"location":"systems/#auto-keras","text":"Auto-Keras focuses on neural architecture search, using bayesian optimization with a custom acquistion function based on network morphism. Presents a high-level API inspired by scikit-learn , where users only need to select a task-specific model; and a low-level API where users can completely customize a neural search space. Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Auto-Keras"},{"location":"systems/#autogoal","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoGOAL"},{"location":"systems/#autogluon","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoGluon"},{"location":"systems/#autonlp","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoNLP"},{"location":"systems/#autopytorch","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoPytorch"},{"location":"systems/#autosklearn","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoSklearn"},{"location":"systems/#autoweka","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoWeka"},{"location":"systems/#azure-ml","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Azure ML"},{"location":"systems/#h2o-automl","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"H2O AutoML"},{"location":"systems/#hyperopt-sklearn","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Hyperopt-Sklearn"},{"location":"systems/#knime-automl","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"KNIME AutoML"},{"location":"systems/#ml-plan","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"ML-Plan"},{"location":"systems/#recipe","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"RECIPE"},{"location":"systems/#tpot","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"TPOT"},{"location":"systems/#transmogrifai","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"TransmogrifAI"},{"location":"systems/#vertex-ai","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Vertex AI"},{"location":"systems_list/","text":"AI Builder \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Auto-Keras \u00b6 Auto-Keras focuses on neural architecture search, using bayesian optimization with a custom acquistion function based on network morphism. Presents a high-level API inspired by scikit-learn , where users only need to select a task-specific model; and a low-level API where users can completely customize a neural search space. Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoGOAL \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoGluon \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoNLP \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoPytorch \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoSklearn \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoWeka \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Azure ML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture H2O AutoML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Hyperopt-Sklearn \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture KNIME AutoML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture ML-Plan \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture RECIPE \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture TPOT \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture TransmogrifAI \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Vertex AI \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Systems list"},{"location":"systems_list/#ai-builder","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AI Builder"},{"location":"systems_list/#auto-keras","text":"Auto-Keras focuses on neural architecture search, using bayesian optimization with a custom acquistion function based on network morphism. Presents a high-level API inspired by scikit-learn , where users only need to select a task-specific model; and a low-level API where users can completely customize a neural search space. Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Auto-Keras"},{"location":"systems_list/#autogoal","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoGOAL"},{"location":"systems_list/#autogluon","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoGluon"},{"location":"systems_list/#autonlp","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoNLP"},{"location":"systems_list/#autopytorch","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoPytorch"},{"location":"systems_list/#autosklearn","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoSklearn"},{"location":"systems_list/#autoweka","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoWeka"},{"location":"systems_list/#azure-ml","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Azure ML"},{"location":"systems_list/#h2o-automl","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"H2O AutoML"},{"location":"systems_list/#hyperopt-sklearn","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Hyperopt-Sklearn"},{"location":"systems_list/#knime-automl","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"KNIME AutoML"},{"location":"systems_list/#ml-plan","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"ML-Plan"},{"location":"systems_list/#recipe","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"RECIPE"},{"location":"systems_list/#tpot","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"TPOT"},{"location":"systems_list/#transmogrifai","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"TransmogrifAI"},{"location":"systems_list/#vertex-ai","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Vertex AI"}]}